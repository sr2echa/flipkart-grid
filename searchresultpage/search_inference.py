# search_inference.py

"""
Grid 7.0 - FAISS Product Search Inference System
===============================================

This script provides semantic search functionality using the pre-built FAISS index.
Load the index files generated by the builder and perform fast similarity searches.

Required Files in the index directory (e.g., ./faiss_index):
- product_index.faiss
- product_id_mapping.json
- product_metadata.pkl
- embedding_stats.json (optional)
"""

import os
import json
import pickle
import time
import logging
import sys
from typing import List, Dict, Any, Optional, Tuple
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_inference_dependencies():
    """Check if all required packages are available for inference."""
    required_packages = {
        'sentence_transformers': 'sentence-transformers',
        'faiss': 'faiss-cpu',
        'numpy': 'numpy'
    }
    missing_packages = []
    for package, pip_name in required_packages.items():
        try:
            # Special handling for faiss as it can be faiss-cpu or faiss-gpu
            if package == 'faiss':
                import faiss
            else:
                __import__(package)
            logger.info(f"✅ {package} is available")
        except ImportError:
            logger.error(f"❌ {package} is missing. Please install: pip install {pip_name}")
            missing_packages.append(pip_name)
    
    if missing_packages:
        return False
    return True

class FAISSProductSearcher:
    """
    FAISS-based product search system that provides fast semantic search 
    over the product catalog.
    """
    
    def __init__(self, index_dir: str = "./faiss_index"):
        """Initializes the search system by loading all necessary files."""
        self.index_dir = index_dir
        self.index = None
        self.product_ids = None
        self.metadata = None
        self.stats = None
        self.sbert_model = None
        self.model_name = 'all-MiniLM-L6-v2'  # Default model

        logger.info("🔍 Grid 7.0 - FAISS Product Search System")
        logger.info("=" * 50)
        
        if not check_inference_dependencies():
            raise RuntimeError("Required dependencies are missing. Please install them.")
        
        self._load_all_components()
        
        logger.info("✅ Search system ready!")
        logger.info(f"📊 Loaded {len(self.product_ids):,} products using '{self.model_name}'")
        logger.info("=" * 50)
    
    def _load_all_components(self):
        """Loads the FAISS index, mappings, metadata, and SBERT model."""
        logger.info(f"📂 Loading index files from: {self.index_dir}")
        if not os.path.exists(self.index_dir):
            raise FileNotFoundError(f"Index directory not found: {self.index_dir}")

        # Load stats first to get model name
        stats_path = os.path.join(self.index_dir, "embedding_stats.json")
        if os.path.exists(stats_path):
            with open(stats_path, 'r', encoding='utf-8') as f:
                self.stats = json.load(f)
            self.model_name = self.stats.get('model_name', self.model_name)
            logger.info(f"✅ Build stats loaded - Model specified: {self.model_name}")
        else:
            logger.warning(f"⚠️ Stats file not found, using default model: {self.model_name}")

        # Load SBERT model
        from sentence_transformers import SentenceTransformer
        logger.info(f"🧠 Loading SBERT model: {self.model_name}")
        self.sbert_model = SentenceTransformer(self.model_name)

        # Load FAISS index
        import faiss
        index_path = os.path.join(self.index_dir, "product_index.faiss")
        self.index = faiss.read_index(index_path)
        logger.info(f"✅ FAISS index loaded with {self.index.ntotal:,} vectors.")

        # Load product ID mapping
        mapping_path = os.path.join(self.index_dir, "product_id_mapping.json")
        with open(mapping_path, 'r', encoding='utf-8') as f:
            self.product_ids = json.load(f)
        logger.info(f"✅ Product ID mapping loaded for {len(self.product_ids):,} products.")

        # Load metadata
        metadata_path = os.path.join(self.index_dir, "product_metadata.pkl")
        with open(metadata_path, 'rb') as f:
            self.metadata = pickle.load(f)
        logger.info(f"✅ Product metadata loaded for {len(self.metadata):,} products.")

    def search(self, query: str, top_k: int = 100) -> List[Dict[str, Any]]:
        """Performs semantic search for a given query."""
        if not query or not query.strip():
            raise ValueError("Search query cannot be empty")
        
        logger.info(f"🔍 Searching for: '{query}' (top {top_k})")
        start_time = time.time()
        
        # Encode query and search FAISS
        query_embedding = self.sbert_model.encode([query.strip()], normalize_embeddings=True)
        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # Prepare results
        results = []
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            if idx == -1: continue

            product_id = self.product_ids[idx]
            product_data = self.metadata.get(product_id, {})
            
            # Convert L2 distance to a more intuitive similarity score (0-1)
            similarity_score = max(0, 1 - (dist / 2))
            
            results.append({
                'rank': i + 1,
                'product_id': product_id,
                'title': product_data.get('title', 'N/A'),
                'brand': product_data.get('brand', 'N/A'),
                'category': product_data.get('category', 'N/A'),
                'price': product_data.get('price', 0),
                'similarity_score': round(similarity_score, 4)
            })
            
        search_time = time.time() - start_time
        logger.info(f"⚡ Search completed in {search_time:.3f}s")
        return results

def create_search_interface(searcher: FAISSProductSearcher):
    """Creates an interactive command-line interface for searching."""
    print("\n🛒 Grid 7.0 - Interactive Product Search")
    print("=" * 50)
    
    while True:
        try:
            query = input("🔍 Enter search query (or 'quit' to exit) > ").strip()
            if query.lower() in ['quit', 'exit', 'q']:
                print("👋 Goodbye!")
                break
            if not query: continue

            results = searcher.search(query, top_k=100)
            
            if not results:
                print("😕 No results found.")
                continue

            print(f"\n📋 Found {len(results)} results:")
            print("-" * 50)
            for res in results:
                print(f"  {res['rank']}. {res['title']} (Brand: {res['brand']})")
                print(f"     Price: ₹{res['price']:.2f} | Similarity: {res['similarity_score']:.3f}")
                print(f"     ID: {res['product_id']}")
            print("-" * 50)

        except KeyboardInterrupt:
            print("\n👋 Goodbye!")
            break
        except Exception as e:
            print(f"❌ An error occurred: {e}")

def main():
    """Main function to initialize and run the search interface."""
    try:
        search_system = FAISSProductSearcher(index_dir="./faiss_index")
        create_search_interface(search_system)
    except Exception as e:
        logger.error(f"Failed to start the search system: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()